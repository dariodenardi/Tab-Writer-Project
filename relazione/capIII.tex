\section{Librerie utilizzate}
\begin{itemize}
	\item \textit{NumPy} è una libreria che aggiunge supporto a grandi matrici e array multidimensionali insieme a una vasta collezione di funzioni matematiche di alto livello per poter operare efficientemente su queste strutture dati;
	\item \textit{Jams} è una libreria che legge file JSON inerenti al mondo della musica;
	\item \textit{Librosa} è una libreria per la musica e l'analisi audio. Fornisce gli elementi necessari per recuperare informazioni musicali;
	\item \textit{SciPy} è una libreria di algoritmi e strumenti matematici che contiene moduli per l'ottimizzazione, per l'algebra lineare, elaborazione di segnali ed immagini e altro;
	\item \textit{Matplotlib} è una libreria per la creazione di grafici;
	\item \textit{Tensorflow} è una libreria utilizzata per il machine learning che fornisce moduli sperimentati e ottimizzati, utili nella realizzazione di algoritmi per diversi tipi di compiti percettivi e di comprensione del linguaggio.
	\item \textit{Keras} consente di implementazione algoritmi basati su reti neurali. Permette di sviluppare e prototipare in maniera semplice e veloce modelli nell’ambito del machine learning e del deep learning. Supporta come back-end \textit{Tensorflow} ed è integrata in essa dalla versione 2.
\end{itemize}

\section{Set dati GuitarSet}
Fortunatamente, su Internet abbiamo trovato un \textit{data set} di file audio di chitarra già pronto su cui lavorare. Il \textit{GuitarSet}, chiamato così dal suo creatore, è costituito dai file audio e dai suoi \textbf{tab}.\\
Questo \textit{data set} contiene 360 estratti di canzoni della durata di circa 30 secondi l'uno. Essi sono il risultato delle seguenti combinazioni:
\begin{itemize}
	\item 6 persone suonano ciascuno gli stessi 30 fogli
	\item Vengono registrate 2 versioni diverse: comping e soloing
\end{itemize}
I 30 fogli sono generati da una combinazione di
\begin{itemize}
	\item \textbf{5 stili}: Rock, Cantautore, Bossa Nova, Jazz e Funk
	\item \textbf{3 progressioni}: 12 Bar Blues, Autumn Leaves e Pachelbel Canon.
	\item \textbf{2 Tempi}: lento e veloce.
\end{itemize}
Gli estratti sono registrati sia con il pickup esafonico che con un microfono a condensatore Neumann U-87.
Ci sono tre registrazioni audio per ogni estratto:
\begin{itemize}
	\item \textbf{hex}: file wav originale a 6 canali dal pickup esafonico;
	\item \textbf{hex\_cln}: file wav esadecimali con rimozione delle interferenze applicata;
	\item \textbf{mic}: registrazione monofonica dal microfono di riferimento
\end{itemize}
Noi abbiamo usato registrazioni di tipo \textbf{mic} perchè sono quelle che più si avvicinano al caso delle registrazioni tramite microfono dello smartphone.\\
\newline
Ciascuno dei 360 estratti ha anche un file .jams che memorizza 16 annotazioni da cui prenderemo le tab:
\begin{itemize}
	\item Intonazione:
	\begin{itemize}
		\item 6 annotazioni \textit{pitch\_contour} (1 per stringa);
		\item 6 annotazioni \textit{midi\_note} (1 per stringa);
	\end{itemize}
	\item Beat e tempo:
	\begin{itemize}
		\item 1 annotazione \textit{beat\_position};
		\item 1 annotazione del tempo;
	\end{itemize}
	\item Accordi:
	\begin{itemize}
		\item 2 annotazioni di accordi (istruite ed eseguite).
	\end{itemize}
\end{itemize}
Noi useremo le annotazioni \textit{midi\_note}.
\subsection{Ricavare le tab dai file .jams}
Innanzitutto calcoliamo i \textit{frame} per ogni \textit{file} audio, così da poter ricavare un immagine e la corrispondente \textit{tab} per ogni \textit{frame}. Per calcolare l'istante di tempo per ogni \textit{frame} utilizziamo la funzione \textit{get\_times()}
\vspace*{2ex}
\pythonexternal{./codes/times.py}
\vspace*{2ex}
Adesso che, per ogni file audio, abbiamo una divisione in frame di cui conosciamo gli istanti di tempo esatti, possiamo estarre dai file .\textit{jams} del \textit{dataset} le tab corrispondenti.\\ Più precisamente, dai file .\textit{jams} prendiamo le \textit{MIDI} note e creiamo una matrice 6x19 dove il 6 rappresenta il numero di corde mentre il 19 rappresenta il numero di tasti.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.30]{./images/img12.jpg}
\end{figure}
Ogni tasto della matrice ha un valore \textit{MIDI} che varia da 40 a 82.
\vspace*{2ex}
\pythonexternal{./codes/matrixMidi.py}
\vspace*{2ex}
Ad ogni matrice sostituiamo i \textit{MIDI} valori con una matrice della stessa dimensione in cui ci sono solo 1 o 0 a seconda dei \textit{MIDI} valori che il \textit{file} .\textit{jams} ci ha restituito. Ad esempio, se dal file .\textit{jams} abbiamo che in quel frame sono state suonate le note 40, 62 e 65 avremmo la seguente matrice:
\vspace*{2ex}
\pythonexternal{./codes/matrixUniZeri.py}
\vspace*{2ex}
Dato che una nota si può trovare su più corde, alla fine bisogna sceglierne soltanto una perchè il suono è lo stesso. Dunque, gli "1" in più devono essere cancellati. Inoltre, siccome queste matrici che chiameremo \textit{labels} le daremo in input al modello è importante avere i dati categorici, quindi devono essere mappati come numeri interi per cui useremo la codifica \textit{one-hot}, cioè si può trovare un "1" solo in ogni riga. A questa matrice aggiungiamo altre due colonne che servono a capire se la corda è stata suonata (colonna 0) e se si, se è stata suonata a vuoto (colonna 1) oppure no.\\
\newline
Di conseguenza, la matrice finale di dimensione 6x21 è la seguente:
\vspace*{2ex}
\pythonexternal{./codes/matrixUniZeriFinale.py}
\vspace*{2ex}
Il codice che esegue quanto abbiamo appena descritto è il seguente:
\vspace*{2ex}
\pythonexternal{./codes/labels.py}
\subsection{Ricavare le immagini dai file audio}
Dopo aver trovato i \textit{label} per ogni \textit{frame}, dobbiamo trovare un modo per far imparare al modello che un frammento di audio sia associato al corrispondente \textit{label}.
Grazie a \textit{librosa} possiamo convertire i \textit{file} audio in immagini. Per far ciò usiamo la trasformazione a Q Costante che ci viene fornita da questa libreria e che ricava per ogni file audio un'immagine.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.90]{./images/img7.png}
\end{figure}
\vspace*{2ex}
\pythonexternal{./codes/librosa.py}
\vspace*{2ex}
I dati (\textit{images} e \textit{labels}) di ogni file audio sono stati compressi in archivi (.npz) per organizzare meglio il \textit{dataset} da dare come input al modello.
\subsection{Pre-elaborare i dati}
Carichiamo i file .npz e per ogni immagine aggiungiamo 4 zeri nell'array dell'immagine sia all'inizio che alla fine. Questo perchè per ogni frame salviamo 9 righe alla volta che corrispondono a 0.2 secondi.\\
Sperimentalmente, abbiamo visto che per ottenere un buon valore di accuratezza nella rete, le immagini devono essere stampate in questo arco temporale. Se il tempo fosse stato inferiore avremmo avuto immagini di file audio con solo note singole e quando la rete avrebbe dovuto riconoscere più note non sarebbestata in grado di farlo.  Nell’immagine seguente è possibile vedere che le lettere sulle note si ripetono.  Ad esempio, la lettera F si trova sulla corda F e D. Se la mano è posizionata sulla corda A, è impossibile che si riesca a suonare la F della corda G. Dunque, un istante di tempo troppo corto non aiutava la rete a riconoscerela nota giusta.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.30]{./images/img12.jpg}
\end{figure}
Il codice che esegue quanto abbiamo appena descritto è il seguente:
\vspace*{2ex}
\pythonexternal{./codes/preElaborazionedati1.py}
\vspace*{2ex}
Una volta ottenuto l'array di tutte le immagini e i label dei file audio lo mescoliamo in modo da avere imprevedibilità. Infine, suddividiamo questi dati nel seguente modo:
\begin{itemize}
	\item 10\% dei dati li usiamo per la \textit{validation}
	\item 10\% dei dati li usiamo per i \textit{test}
	\item 80\% dei dati li usiamo per il \textit{traning}
\end{itemize}
Per costruire il modello usiamo il \textit{traning set}, il validation set per validarlo mentre il \textit{test set} per determinare l'accuratezza.\\
\newline
Il codice che esegue quanto abbiamo appena descritto è il seguente:
\vspace*{2ex}
\pythonexternal{./codes/preElaborazionedati2.py}
\vspace*{2ex}
\section{Modello della rete}
La difficoltà nell’apprendere i meccanismi di implementazione su Keras sono ridotti al minimo grazie alla vasta documentazione presente, arricchita da numerosi esempi sulle più utilizzate configurazioni inerenti il machine learning, come CNN (Convolutional Neural Network).
Le operazioni di calcolo matriciali possono essere accelerate sia tramite CPU, che GPU (su hardware Nvidia con supporto CUDA).
Le caratteristiche e i vantaggi che ci hanno portato ad utilizzarlo nell’ambito del progetto sono:
\begin{itemize}
	\item Semplicità: a differenza di altre API, è possibile realizzare modelli complessi scrivendo meno righe di codice, mantenendo nel contempo chiarezza nello sviluppo. Tutto ciò consente allo sviluppatore di mantenere nel tempo il codice in maniera agevole.
	\item Modularità: un modello in Keras è inteso come una sequenza o un grafo di singoli, compatti e completamente configurabili moduli, che possono lavorare in sinergia tra loro con il minimo numero di restrizioni possibili. Ciò rende il codice estremamente flessibile.
	\item Estensibilità: in base alle esigenze dello sviluppatore, è possibile aggiungere facilmente nuovi moduli (ad esempio classi e funzioni) ad un progetto preesistente.
\end{itemize}
\subsection{Uso di Keras}
Prima di dare in input al modello traning set e validation set, dobbiamo fare delle modifiche alla dimensione del numpy array di nome X (images) in quanto il modello si aspetta un input di BATCHx192x9x1.
\begin{itemize}
	\item batch sono la quantità di valori dell'intero traning set;
	\item 192 è l'altezza dell'immagine;
	\item 9 è la lunghezza dell'immagine;
	\item 1 ci indica che l'immagine è in bianco e nero.
\end{itemize}
La Y non ha bisogno di modifiche perchè le dimensioni sono già quelle corrette cioè ha dimensione BATCHx21x6.\\
Il codice che esegue quanto abbiamo appena descritto è il seguente:
\vspace*{2ex}
\pythonexternal{./codes/modelloFinale1.py}
\vspace*{2ex}
A questo punto definiamo il modello:
\begin{itemize}
	\item \textbf{Conv2D}: serve per mettere in evidenza le caratteristiche interessanti dell'immagine. parametri di input: numero di filtri, grandezza filtri, input shape e funzione di attivazione.
	\item \textbf{MaxPooling2D}: riduce la dimensione dell'immagine, elimina le informazioni inutili mantenendo quelle più importanti. parametri di input: matrice di input;
	\item \textbf{Dropout} elimina una percentuale di dati casuali. Ad esempio, elimina rumori di sottofondo e mantiene le informazioni più importanti. Parametri di input: percentuale.
	\item \textbf{Flattern} appiattisce il tensore e rimuove tutte le dimensioni;
	\item \textbf{Dense}: decide il numero di numero di neuroni in uscita.
	\item \textbf{Reshape}: determina la dimensione di uscita. In questo caso è 6x21;
	\item \textbf{Activation} serve affinchè la somma di ogni elemento di uscita sia uno.
\end{itemize}
Per compilare il modello abbiamo scelto come ottimizzatore l'algoritmo Adadelta che utilizza un metodo di discesa del gradiente stocastico basato sul tasso di apprendimento adattivo per dimensione per affrontare l'inconveniente del continuo declino dei tassi di apprendimento durante la formazione e la necessità di un tasso di apprendimento globale selezionato manualmente.
Il codice che esegue quanto abbiamo appena descritto è il seguente:\\
\newline
\vspace*{2ex}
\pythonexternal{./codes/modelloFinale3.py}
\vspace*{2ex}
Questo è il \textit{summary} del modello:
\vspace*{2ex}
\pythonexternal{./codes/modelloFinale5.py}
\vspace*{2ex}
In particolare, per il modello abbiamo usato delle funzioni personalizzate per la funzione di attivazione finale del modello (\textit{sofmax\_by\_string}), per la funzione obiettivo \textit{loss} (\textit{catcross\_by\_string}) e per le metriche (\textit{avg\_acc}) che devono essere valutate del modello durante l'addestramento e il modello.\\
\newline
Il codice che esegue quanto abbiamo appena descritto è il seguente:
\vspace*{2ex}
\pythonexternal{./codes/modelloFinale2.py}
\vspace*{2ex}
Per avviare l'addestramento del modello eseguiamo il comando \textit{model.fit()} dove indichiamo con \textit{batch\_size} il numero di campioni per ogni aggiornamento del gradiente e con \textit{epochs} il numero di iterazioni sul quale il modello deve effettuare il \textit{traning}.\\
\newline
Il codice che esegue quanto abbiamo appena descritto è il seguente:
\vspace*{2ex}
\pythonexternal{./codes/modelloFinale4.py}
\vspace*{2ex}
\subsection{Addestramento del modello}
Dopo diverse prove sperimentali, abbiamo deciso di eseguire il modello per 500 epoche:
\vspace*{2ex}
\pythonexternal{./codes/storia.py}
\vspace*{2ex}
Questo modello raggiunge una precisione di circa 0,87 (o 87\%) e una perdita del 2\% sui dati di addestramento. Invece, sui dati di validazione la precisione supera lo 0,90 (90\%) e raggiunge una perdita del 1,6\% sui dati di validazione. I \textit{loss} sono la media delle perdite sui dati di \textit{batch} di addestramento. Con questi dati sembra avere ottimi risultati in quanto non abbiamo la presenza di \textit{overfitting}.\\
\newline
Il grafico sottostante ci fa comprendere meglio i risultati:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.50]{./images/plot.png}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.50]{./images/plot2.png}
\end{figure}

\section{Valutazione del modello}
Abbiamo eseguito diversi test per mettere alla prova il modello utilizzando il \textit{training test}. La matrice "Answer" è la y corrispondente alla X data come input alla predict. Invece, la "Prediction" corrisponde all'output della rete. Il valore più grande della riga corrisponde a dove secondo il modello ci debba essere l'1.\\
\newline
Di seguito riportiamo un esempio:
\vspace*{2ex}
\pythonexternal{./codes/test.py}
\vspace*{2ex}
Abbiamo confrontato le prestazioni del modello sul set di dati di test e i risultati sono stati quelli previsti. Infatti, l'accuratezza e la perdita dei dati di test sono molto simili a quelli dei dati di validazione.
\vspace*{2ex}
\pythonexternal{./codes/valutazione.py}
\vspace*{2ex}

Il codice che esegue quanto abbiamo appena descritto è il seguente:
\vspace*{2ex}
\pythonexternal{./codes/accuratezza.py}
\vspace*{2ex}